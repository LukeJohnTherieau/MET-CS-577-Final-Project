{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce3f728a",
   "metadata": {},
   "source": [
    "https://cseweb.ucsd.edu/~jmcauley/datasets.html#amazon_reviews\n",
    "\n",
    "https://amazon-reviews-2023.github.io/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7ee55b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4138f780",
   "metadata": {},
   "source": [
    "# Citation\n",
    "Bridging Language and Items for Retrieval and Recommendation\n",
    "Yupeng Hou, Jiacheng Li, Zhankui He, An Yan, Xiusi Chen, Julian McAuley\n",
    "arXiv\n",
    "[pdf](https://urldefense.com/v3/__https://arxiv.org/pdf/2403.03952.pdf__;!!Mih3wA!EGkx27nmvdVMhh2uxQ7Mc0rNrXQwV8GsOpd3uSc6ZjJGAVhgy9o5bn3Jeb73P4Lz7oL2dIDMdZR4IHI$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a70d38f",
   "metadata": {},
   "source": [
    "https://scikit-learn.org/0.15/modules/scaling_strategies.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109b7176",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pymongo\n",
    "!pip install pandas\n",
    "!pip install bs4\n",
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c83163b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MongoDB import MongoDB\n",
    "import pandas as pd\n",
    "from multiprocessing import Process\n",
    "\n",
    "\n",
    "def load_amazon_data(mongo_db:MongoDB, amazon_reviews_db:MongoDB, category:str, url:str):\n",
    "    if category not in mongo_db:\n",
    "        print(category, mongo_db)\n",
    "        df = pd.read_json(\n",
    "            path_or_buf = url, \n",
    "            compression = \"gzip\",\n",
    "            lines =\n",
    "            True\n",
    "        )\n",
    "        Process(\n",
    "            target = mongo_db.append,\n",
    "            args = (\n",
    "                category,\n",
    "                df,\n",
    "                (\n",
    "                    {\"timeField\" : \"timestamp\"} \n",
    "                    if mongo_db == amazon_reviews_db\n",
    "                    else {}\n",
    "                ),\n",
    "            )\n",
    "        ).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60d82776",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "from MongoDB import MongoDB\n",
    "\n",
    "\n",
    "amazon_reviews_db = MongoDB(\"Amazon_Reviews\")\n",
    "amazon_items_db = MongoDB(\"Amazon_Items\")\n",
    "response = requests.get(\"https://amazon-reviews-2023.github.io/\")\n",
    "soup  = BeautifulSoup(response.text, \"html.parser\")\n",
    "file_links = soup.find_all(\n",
    "    name = \"a\",\n",
    "    attrs = {\n",
    "        \"href\" : re.compile(r'.*https://mcauleylab.ucsd.edu/public_datasets/data/amazon_2023/raw/.*')\n",
    "    }\n",
    ")\n",
    "for file_link in file_links:\n",
    "    categories = file_link.attrs[\"href\"].split(\"/\")[-1].split(\".jsonl.gz\")[0].partition(\"meta_\")\n",
    "    if categories[0] == \"Video_Games\":# or categories[-1] == \"Video_Games\":\n",
    "        print(categories)\n",
    "        mongo_db = amazon_reviews_db if categories[-1] == \"\" else amazon_items_db\n",
    "        category = categories[0] if categories[-1] == \"\" else categories[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c32a4870",
   "metadata": {},
   "outputs": [],
   "source": [
    "len([\n",
    "    file_link\n",
    "    for file_link \n",
    "    in file_links\n",
    "    if \"meta\" not in file_link.attrs[\"href\"]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ecf4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "ROOT = \"data/\"\n",
    "\n",
    "\n",
    "df = pd.concat(\n",
    "    objs = [\n",
    "        pd.read_feather(ROOT + file)\n",
    "        for file \n",
    "        in os.listdir(ROOT)\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba659335",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords_list\n",
    "\n",
    "df[\"text_tokenized\"] = df[\"text\"].apply(\n",
    "    lambda text: \" \".join(\n",
    "        [\n",
    "            token\n",
    "            for token \n",
    "            in nltk.word_tokenize(text)\n",
    "            if token not in exclude_lists\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "for text in reviews[\"text_tokenized\"].values:\n",
    "    for token in text.split(\" \"):\n",
    "        vocabulary.add(token)\n",
    "    \n",
    "# temp_vectorizer = CountVectorizer()\n",
    "# temp_vectorizer.fit(reviews[\"text_tokenized\"])\n",
    "# print(temp_vectorizer.vocabulary_.keys())\n",
    "# vocabulary.update(temp_vectorizer.vocabulary_.keys())\n",
    "        \n",
    "reviews[\"sentiment\"] = reviews[\"rating\"].apply(\n",
    "    lambda rating: 1 if rating > 3 else 0 if rating < 3 else -1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc74993b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90c377aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews_db = MongoDB(\"Amazon_Reviews\") = MongoDB(\"Amazon_Reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f816ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for df in amazon_reviews_db.query(\"Video_Games\",{}):\n",
    "    count += len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93f59348",
   "metadata": {},
   "outputs": [],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3a91d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "multiprocessing.cpu_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb4a116",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install wordcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a58a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords_list\n",
    "import os\n",
    "\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm') \n",
    "exclude_lists = list(stopwords_list) + list(string.punctuation)\n",
    "ROOT = \"data/\"\n",
    "df = pd.concat(\n",
    "    objs = [\n",
    "        pd.read_feather(ROOT + file)\n",
    "        for file \n",
    "        in os.listdir(ROOT)\n",
    "    ]\n",
    ")\n",
    "df[\"sentiment\"] = df[\"rating\"].apply(\n",
    "    lambda rating: 1 if rating > 3 else 0 if rating < 3 else -1\n",
    ")\n",
    "df.query(\"sentiment != -1\", inplace = True)\n",
    "df = df.iloc[:100_000]\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1642f71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\", disable=['parser', 'tagger', 'ner'])\n",
    "stops = spacy.lang.en.stop_words.STOP_WORDS\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def normalize(col, lowercase, remove_stopwords):\n",
    "    print(col.name)\n",
    "    comment = col[\"text\"]\n",
    "    if lowercase:\n",
    "        comment = comment.lower()\n",
    "    comment = nlp(comment)\n",
    "    lemmatized = list()\n",
    "    for word in comment:\n",
    "        lemma = word.lemma_.strip()\n",
    "        if lemma:\n",
    "            if not remove_stopwords or (remove_stopwords and lemma not in stops):\n",
    "                lemmatized.append(lemma)\n",
    "    return \" \".join(lemmatized)\n",
    "\n",
    "\n",
    "df['cleaned_text'] = df.apply(normalize, lowercase=True, remove_stopwords=True, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558e069f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     df[\"text\"], \n",
    "     df[\"sentiment\"], \n",
    "     test_size = 0.3, \n",
    "     random_state = 34,\n",
    "     stratify = df[\"sentiment\"]\n",
    ")\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, GaussianNB, BernoulliNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer(\n",
    "    stop_words = exclude_lists\n",
    ")\n",
    "clf = BernoulliNB().fit(count_vect.fit_transform(X_train.values).toarray(), y_train) \n",
    "clf.score(count_vect.transform(X_test.values).toarray(), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100e70c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "log_probabilities = clf.feature_log_prob_\n",
    "feature_names = count_vect.get_feature_names_out()\n",
    "\n",
    "for sentiment, color in zip([0, 1], [\"Reds\", \"Greens\"]):\n",
    "    log_probabilities_sentiment = sorted(log_probabilities[sentiment,:])[::-1]\n",
    "    features_sentiments = [feature_names[i] for i in log_probabilities[sentiment,:].argsort()[::-1]]\n",
    "    \n",
    "    # Create a WordCloud object with desired parameters\n",
    "    wordcloud = WordCloud(\n",
    "        width = 800, \n",
    "        height = 400, \n",
    "        background_color = \"white\",\n",
    "        max_words = 100,\n",
    "        colormap = color\n",
    "    ).generate_from_frequencies(\n",
    "        {\n",
    "            feature: prob  \n",
    "            for feature, prob \n",
    "            in zip(features_sentiments, log_probabilities_sentiment)\n",
    "        }\n",
    "    )\n",
    "\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "    plt.axis(\"off\") # Turn off the axis labels and ticks\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7981f3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db8ca781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a10928e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Generator\n",
    "import pandas as pd\n",
    "import pymongo\n",
    "\n",
    "\n",
    "class MongoDB:\n",
    "    def __init__(self, database_name:str, url:str = \"mongodb://localhost:27017/\"):\n",
    "        self.__database_name = database_name\n",
    "        self.__url = url\n",
    "        \n",
    "        \n",
    "    def append(self, df:pd.DataFrame, collection_name:str, timeseries:dict = None) -> bool:\n",
    "        if not timeseries:\n",
    "            timeseries = {}\n",
    "            \n",
    "        else:\n",
    "            timeseries = {\n",
    "                \"timeseries\" : timeseries\n",
    "            }\n",
    "            \n",
    "        with pymongo.MongoClient(self.__url) as client:\n",
    "            db = client[self.__database_name]\n",
    "            if collection_name not in db.list_collection_names():\n",
    "                db.create_collection(**{\"name\" : collection_name} | timeseries)\n",
    "                \n",
    "            db[collection_name].insert_many(\n",
    "                df.to_dict(\n",
    "                    orient = \"records\"\n",
    "                )\n",
    "            )\n",
    "            \n",
    "            \n",
    "    def count(self, collection_name:str, query:dict = {}) -> int:\n",
    "        with pymongo.MongoClient(self.__url) as client:\n",
    "            db = client[self.__database_name]\n",
    "            return db[collection_name].count_documents(query)\n",
    "        \n",
    "        \n",
    "    def list_collection_names(self):\n",
    "        with pymongo.MongoClient(self.__url) as client:\n",
    "            db = client[self.__database_name]\n",
    "            return db.list_collection_names()\n",
    "        \n",
    "        \n",
    "    def query(self, collection_name:str, query:dict = {}):\n",
    "        rows = []\n",
    "        with pymongo.MongoClient(self.__url) as client:\n",
    "            db = client[self.__database_name]\n",
    "            cursor  = db[collection_name].find(\n",
    "                filter = query\n",
    "            )\n",
    "            previous_retrieved = 0\n",
    "            for row in cursor:\n",
    "                if len(row) > 0 and cursor.retrieved != previous_retrieved:\n",
    "                    previous_retrieved = cursor.retrieved\n",
    "                    yield pd.DataFrame(rows)\n",
    "                    rows.clear()\n",
    "                        \n",
    "                rows.append(row)\n",
    "                \n",
    "            yield pd.DataFrame(rows)\n",
    "            \n",
    "            \n",
    "    def aggregate(self, collection_name:str, pipeline:list):\n",
    "        rows = []\n",
    "        with pymongo.MongoClient(self.__url) as client:\n",
    "            db = client[self.__database_name]\n",
    "            cursor  = db[collection_name].aggregate(pipeline)\n",
    "            return pd.DataFrame(\n",
    "                [\n",
    "                    row\n",
    "                    for row \n",
    "                    in cursor\n",
    "                ]\n",
    "            )\n",
    "    \n",
    "    \n",
    "    def __contains__(self, collection_name):\n",
    "        with pymongo.MongoClient(self.__url) as client:\n",
    "            db = client[self.__database_name]\n",
    "            return collection_name in db.list_collection_names()\n",
    "        \n",
    "        \n",
    "    def __repr__(self):\n",
    "        return self.__database_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bf35fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MongoDB import MongoDB\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "amazon_reviews_db = MongoDB(\"Amazon_Reviews\")\n",
    "amazon_items_db = MongoDB(\"Amazon_Items\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "459304fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for collection_name in amazon_reviews_db.list_collection_names():.count(collection_name)\n",
    "#     print(collection_name, amazon_reviews_db.count(collection_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906fee80",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = \"\"\"\n",
    "Pet_Supplies 360000\n",
    "system.buckets.Pet_Supplies 201892\n",
    "Cell_Phones_and_Accessories 260000\n",
    "system.buckets.Cell_Phones_and_Accessories 159883\n",
    "Electronics 360000\n",
    "system.buckets.Electronics 206652\n",
    "CDs_and_Vinyl 400000\n",
    "system.buckets.CDs_and_Vinyl 251001\n",
    "Baby_Products 320000\n",
    "system.buckets.Baby_Products 189706\n",
    "Amazon_Fashion 2500939\n",
    "system.buckets.Amazon_Fashion 884152\n",
    "Toys_and_Games 360000\n",
    "system.buckets.Toys_and_Games 198077\n",
    "Magazine_Subscriptions 71497\n",
    "system.buckets.Magazine_Subscriptions 58141\n",
    "Grocery_and_Gourmet_Food 360000\n",
    "system.buckets.Grocery_and_Gourmet_Food 199580\n",
    "Industrial_and_Scientific 290000\n",
    "system.buckets.Industrial_and_Scientific 173895\n",
    "Movies_and_TV 410000\n",
    "system.buckets.Movies_and_TV 222404\n",
    "Patio_Lawn_and_Garden 250000\n",
    "system.buckets.Patio_Lawn_and_Garden 150257\n",
    "Software 370000\n",
    "system.buckets.Software 212219\n",
    "Gift_Cards 152410\n",
    "system.buckets.Gift_Cards 101028\n",
    "Books 270000\n",
    "system.buckets.Books 170023\n",
    "Clothing_Shoes_and_Jewelry 350000\n",
    "system.buckets.Clothing_Shoes_and_Jewelry 187206\n",
    "Health_and_Personal_Care 350000\n",
    "system.buckets.Health_and_Personal_Care 228253\n",
    "Musical_Instruments 310000\n",
    "system.buckets.Musical_Instruments 188044\n",
    "Kindle_Store 280000\n",
    "system.buckets.Kindle_Store 168478\n",
    "Handmade_Products 370000\n",
    "system.buckets.Handmade_Products 220170\n",
    "system.views 34\n",
    "Home_and_Kitchen 340000\n",
    "system.buckets.Home_and_Kitchen 185938\n",
    "Automotive 290000\n",
    "system.buckets.Automotive 169511\n",
    "Subscription_Boxes 16216\n",
    "system.buckets.Subscription_Boxes 13346\n",
    "Health_and_Household 260000\n",
    "system.buckets.Health_and_Household 149124\n",
    "Tools_and_Home_Improvement 270000\n",
    "system.buckets.Tools_and_Home_Improvement 159367\n",
    "Sports_and_Outdoors 280000\n",
    "system.buckets.Sports_and_Outdoors 166159\n",
    "Office_Products 270000\n",
    "system.buckets.Office_Products 161496\n",
    "Video_Games 290000\n",
    "system.buckets.Video_Games 184937\n",
    "Beauty_and_Personal_Care 270000\n",
    "system.buckets.Beauty_and_Personal_Care 152860\n",
    "Unknown 330000\n",
    "system.buckets.Unknown 190146\n",
    "Arts_Crafts_and_Sewing 370000\n",
    "system.buckets.Arts_Crafts_and_Sewing 202562\n",
    "Appliances 340000\n",
    "system.buckets.Appliances 210106\n",
    "All_Beauty 701528\n",
    "system.buckets.All_Beauty 278351\n",
    "Digital_Music 130434\n",
    "system.buckets.Digitcal_Musi 99288\n",
    "\"\"\".split(\"\\n\")[1:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af58c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = pd.DataFrame(\n",
    "    [\n",
    "        [col.split(\" \")[0],float(col.split(\" \")[1])]\n",
    "        for col\n",
    "        in results\n",
    "        if not col.startswith(\"system\")\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f850bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_reviews_db.aggregate(\n",
    "    collection_name = \"Magazine_Subscriptions\",\n",
    "    pipeline = [\n",
    "        {\n",
    "            \"$group\" : {\n",
    "                \"_id\": None, \n",
    "                \"Max Date\": {\n",
    "                    \"$min\": \"$timestamp\"\n",
    "                } \n",
    "            }  \n",
    "        }\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "493f8f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols[0].iloc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1ed23c",
   "metadata": {},
   "source": [
    "https://medium.com/@aleksej.gudkov/llama-cpp-python-examples-a-guide-to-using-llama-models-with-python-1df9ba7a5fcd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1857cb20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords_list\n",
    "\n",
    "def text_filter(a_dict:pd.Series, label:pd.Series, exclude_lists:list):\n",
    "    data = []\n",
    "    for rev_id in a_dict.keys():\n",
    "        tokens = []\n",
    "        for token in a_dict.get(rev_id):\n",
    "            if not token.text in exclude_lists:\n",
    "                tokens.append(token.text)\n",
    "        data.append((' '.join(tokens), label))\n",
    "    return data\n",
    "\n",
    "def prepare_data(pos_docs, neg_docs, exclude_lists):\n",
    "    data = text_filter(pos_docs, 1, exclude_lists)\n",
    "    data += text_filter(neg_docs, -1, exclude_lists)\n",
    "    random.seed(42)\n",
    "    random.shuffle(data)\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for item in data:\n",
    "        texts.append(item[0])\n",
    "        labels.append(item[1])\n",
    "        \n",
    "    return texts, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a6d65c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords_list\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self, exclude_lists:list = None):\n",
    "        if not exclude_lists:\n",
    "            exclude_lists = list(stopwords_list) + list(string.punctuation)\n",
    "            \n",
    "        self.exclude_lists = exclude_lists\n",
    "        \n",
    "        \n",
    "    def filter(self, text:str) -> str:\n",
    "        return \" \".join([\n",
    "                token\n",
    "                for token \n",
    "                in nltk.word_tokenize(text)\n",
    "                if token not in self.exclude_lists\n",
    "        ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f542e65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MongoDB import MongoDB\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "import string\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords_list\n",
    "\n",
    "\n",
    "vocabulary = set()\n",
    "collection_name = \"Amazon_Fashion\"\n",
    "amazon_reviews_db = MongoDB(\"Amazon_Reviews\")\n",
    "exclude_lists = list(stopwords_list) + list(string.punctuation)\n",
    "all_reviews = []\n",
    "for reviews in amazon_reviews_db.query(collection_name, query = {\"timestamp\" : {\"$lt\" : datetime(2015,1,1)}}):\n",
    "    reviews[\"text_tokenized\"] = reviews[\"text\"].apply(\n",
    "        lambda text: \" \".join(\n",
    "            [\n",
    "                token\n",
    "                for token \n",
    "                in nltk.word_tokenize(text)\n",
    "                if token not in exclude_lists\n",
    "            ]\n",
    "        )\n",
    "    )\n",
    "    for text in reviews[\"text_tokenized\"].values:\n",
    "        for token in text.split(\" \"):\n",
    "            vocabulary.add(token)\n",
    "        \n",
    "    # temp_vectorizer = CountVectorizer()\n",
    "    # temp_vectorizer.fit(reviews[\"text_tokenized\"])\n",
    "    # print(temp_vectorizer.vocabulary_.keys())\n",
    "    # vocabulary.update(temp_vectorizer.vocabulary_.keys())\n",
    "            \n",
    "    reviews[\"sentiment\"] = reviews[\"rating\"].apply(\n",
    "        lambda rating: 1 if rating > 3 else 0 if rating < 3 else -1\n",
    "    )\n",
    "    all_reviews.append(reviews)\n",
    "    \n",
    "all_reviews_df = pd.concat(all_reviews, ignore_index=True)\n",
    "all_reviews_df = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83d6a79f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_1 = CountVectorizer(vocabulary=vocabulary)\n",
    "count_vect_1.transform(all_reviews_df[\"text_tokenized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d1e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_2 = CountVectorizer()\n",
    "count_vect_2.fit_transform(all_reviews_df[\"text_tokenized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fa30a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "count_vect_1.transform(all_reviews_df[\"text_tokenized\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30704d0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "     all_reviews_df[\"text_tokenized\"], \n",
    "     all_reviews_df[\"sentiment\"], \n",
    "     test_size = 0.3, \n",
    "     random_state = 34\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8ab675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "count_vect = CountVectorizer()\n",
    "clf = MultinomialNB().fit(count_vect.fit_transform(X_train.values), y_train) \n",
    "clf.score(count_vect.transform(X_test.values), y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae8ef9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cecac7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f289fabf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MongoDB import MongoDB\n",
    "import pandas as pd\n",
    "import en_core_web_sm\n",
    "\n",
    "collection_name = \"Amazon_Fashion\"\n",
    "nlp = en_core_web_sm.load()\n",
    "amazon_reviews_db = MongoDB(\"Amazon_Reviews\")\n",
    "total_reviews = amazon_reviews_db.count(collection_name)\n",
    "\n",
    "i = 0\n",
    "for reviews in amazon_reviews_db.query(collection_name):\n",
    "    print(len(reviews))\n",
    "    reviews[\"sentiment\"] = reviews[\"rating\"].apply(\n",
    "        lambda rating: 1 if rating > 3 else 0 if rating < 3 else -1\n",
    "    )\n",
    "    reviews[\"text\"] = reviews[\"text\"].apply(\n",
    "        lambda text: nlp(\n",
    "            text,\n",
    "            disable = [\"ner\"]\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    if i > 2:\n",
    "        break\n",
    "    \n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17262b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "train_counts = count_vect.fit_transform(train_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cb53b6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c10a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "\n",
    "punctuation_list = [\n",
    "    punct \n",
    "    for punct \n",
    "    in string.punctuation\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45faa8a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, labels = prepare_data(pos_docs, neg_docs, punctuation_list)\n",
    "print(len(texts), len(labels))\n",
    "print(texts[0]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5256be32",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b6b0e1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2912e095",
   "metadata": {},
   "outputs": [],
   "source": [
    "from MongoDB import MongoDB\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "amazon_reviews_db = MongoDB(\"Amazon_Reviews\")\n",
    "amazon_items_db = MongoDB(\"Amazon_Items\")\n",
    "\n",
    "\n",
    "reviews = pd.concat(\n",
    "    objs = [\n",
    "        batch\n",
    "        for batch\n",
    "        in amazon_reviews_db.query(\"All_Beauty\")\n",
    "    ]\n",
    ")\n",
    "reviews[\"sentiment\"] = reviews[\"rating\"].apply(\n",
    "    lambda rating: 1 if rating > 3 else 0 if rating < 3 else -1\n",
    ")\n",
    "items = pd.concat(\n",
    "    objs = [\n",
    "        batch\n",
    "        for batch\n",
    "        in amazon_items_db.query(\"All_Beauty\")\n",
    "    ]\n",
    ")\n",
    "pd.merge(\n",
    "    left = reviews,\n",
    "    right = items,\n",
    "    on = \"parent_asin\",\n",
    "    how = \"left\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8831e11",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
